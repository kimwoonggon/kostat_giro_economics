{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(건설경기동향조사)경인청_건설_민간자료_수집관리시스템.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM24hEyyJyOJK4Zlq7Bt1Yu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimwoonggon/kostat_giro_economics/blob/master/(%EA%B1%B4%EC%84%A4%EA%B2%BD%EA%B8%B0%EB%8F%99%ED%96%A5%EC%A1%B0%EC%82%AC)%EA%B2%BD%EC%9D%B8%EC%B2%AD_%EA%B1%B4%EC%84%A4_%EB%AF%BC%EA%B0%84%EC%9E%90%EB%A3%8C_%EC%88%98%EC%A7%91%EA%B4%80%EB%A6%AC%EC%8B%9C%EC%8A%A4%ED%85%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_snHlT2L5J",
        "colab_type": "code",
        "outputId": "33f274e8-6465-4129-fe56-8811aaf8d70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W83lCbgSviAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 이 버전은 담당자별로 자료를 수집하여 담당자가 조사하는 건설사 별로 자료를 분류\n",
        "## 최종 수정 날짜 : '20. 2. 19\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import collections\n",
        "import glob\n",
        "import numpy as np\n",
        "from multiprocessing import Pool, freeze_support\n",
        "from functools import partial\n",
        "from itertools import repeat\n",
        "\n",
        "path = \"/content/gdrive/My Drive/Colab Notebooks/건설경기동향조사\"\n",
        "class gunseol_crawl:\n",
        "    # api_id, api_password에 네이버 개발자 센터에서 발급받은 api_id , api_password를 입력해 주셔야 사용 가능합니다.\n",
        "\n",
        "    def __init__(self, file_path, api_id='', api_password='', multiprocessing=False):\n",
        "        self.total_data = pd.DataFrame(columns=['날짜', '제목', '기사요약', '링크', '담당자', '사업체명'])\n",
        "        self.file_path = file_path\n",
        "        self.api_id = api_id\n",
        "        self.api_password = api_password\n",
        "        \n",
        "        self.user_search_lists = self.load_gunseol_list()\n",
        "        if multiprocessing is False:\n",
        "            for self.user_search_list in self.user_search_lists:\n",
        "                \n",
        "                print(self.user_search_list[0]+'님의 건설경기 자료를 수집하겠습니다\\n')\n",
        "\n",
        "                print(self.user_search_list)\n",
        "                self.crawl_news_data(self.user_search_list[1:], self.user_search_list[0])\n",
        "                print('%s님 수고하셨습니다.' % self.user_search_list[0])\n",
        "            self.save_to_excel()\n",
        "\n",
        "\n",
        "        else:\n",
        "            with Pool(3) as pool:\n",
        "                for user_search_list in user_search_lists:\n",
        "                    func = partial(self.crawl_news_data, user_name = user_search_list[0])\n",
        "                    pool.map(func, user_search_list[1:])\n",
        "\n",
        "    def load_gunseol_list(self):\n",
        "        gunseol_list = pd.read_csv(self.file_path, encoding='cp949', engine='python')\n",
        "        total_gunseol_list = []\n",
        "        name_list = gunseol_list['담당자'].unique().tolist()\n",
        "        for name in name_list:\n",
        "            gun = gunseol_list.loc[gunseol_list['담당자'] == name_list[name_list.index(name)]]['기업체명'].tolist()\n",
        "            re_gun = []\n",
        "            for i in gun:\n",
        "                del_letter = re.compile('\\((주)\\)')\n",
        "                i = del_letter.sub(\"\", i)\n",
        "\n",
        "                re_gun.append(i)\n",
        "            re_gun.insert(0, name)\n",
        "            total_gunseol_list.append(re_gun)\n",
        "\n",
        "        return total_gunseol_list\n",
        "\n",
        "    def get_naver_connection(self, url):\n",
        "\n",
        "        req = urllib.request.Request(url)\n",
        "        req.add_header('X-Naver-Client-Id', self.api_id)\n",
        "        req.add_header('X-Naver-Client-Secret', self.api_password)\n",
        "\n",
        "\n",
        "        try:\n",
        "            response = urllib.request.urlopen(req)\n",
        "\n",
        "            if response.getcode() == 200:\n",
        "                return response.read().decode('utf-8')\n",
        "\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def get_naver_json(self, kind, search_text, page_start, display_num):\n",
        "        base = 'https://openapi.naver.com/v1/search'\n",
        "        node = '/%s.json' % kind\n",
        "        parameters = '?query=%s&start=%s&display=%s' % (urllib.parse.quote(search_text), page_start, display_num)\n",
        "        url = base + node + parameters\n",
        "\n",
        "\n",
        "        raw_json = self.get_naver_connection(url)\n",
        "\n",
        "        if raw_json == None:\n",
        "            return None\n",
        "        else:\n",
        "            return json.loads(raw_json)\n",
        "\n",
        "    def process_data(self, post, jsonResult):\n",
        "\n",
        "\n",
        "        pDate = datetime.datetime.strptime(post['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')\n",
        "        # 수집 기간은 28일\n",
        "        if (datetime.datetime.now() - pDate).total_seconds() > int(86400 * 7 * 4):\n",
        "            return None\n",
        "        else:\n",
        "            pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            title = post['title']\n",
        "\n",
        "        del_letters1 = re.compile('[<b>|</b>|]')\n",
        "        del_letters2 = re.compile('[&quot]')\n",
        "\n",
        "        title = del_letters1.sub(\"\",title)\n",
        "        title = del_letters2.sub(\"\",title)\n",
        "        description = post['description']\n",
        "        description = del_letters1.sub(\"\",description)\n",
        "        description = del_letters2.sub(\"\",description)\n",
        "\n",
        "        link = post['originallink']\n",
        "\n",
        "        description, title = self.filter_article(description=description, title=title)\n",
        "        if description is not None and title is not None:\n",
        "\n",
        "            sys.stdout.write(title+\"\\n\")\n",
        "            jsonResult.append({'제목':title, '기사요약':description, '링크':link, '날짜':pDate})\n",
        "\n",
        "        return None\n",
        "\n",
        "    def filter_article(self, description, title):\n",
        "        # 아시아, 중동\n",
        "        country_list = ['일본', '중국', '러시아', '대만', '필리핀', '브루나이', '인도네시아', '동티모르', '파푸아뉴기니',\n",
        "                        '싱가포르', '말레이시아', '캄보디아', '베트남', '라오스', '태국', '미얀마', '방글라데시', '부탄', '네팔',\n",
        "                        '스리랑카', '인도', '몰디브', '파키스탄', '카자흐스탄', '키르키스스탄', '우즈베키스탄', '투르크메니스탄',\n",
        "                        '아프가니스탄', '이란', '이라크', '쿠웨이트', '바레인', '아랍에미리트', 'UAE', '시리아', '터키', '레바논',\n",
        "                        '요르단', '이스라엘', '사우디아라비아', '카타르', '오만', '예멘', '이집트', '알제리', '아시안','중동', '동남아', '사우디']\n",
        "\n",
        "        country_list_2 = ['체코', '폴란드', '유럽', '스페인', '이탈리아']\n",
        "\n",
        "        country_list_3 = ['멕시코', '남미', '아프리카', '에티오피아', '나이지리아','파나마', '모잠비크']\n",
        "\n",
        "        filter_s = country_list + country_list_2 + country_list_3\n",
        "        for filter in filter_s:\n",
        "            if filter in description or filter in title:\n",
        "\n",
        "                return None, None\n",
        "\n",
        "        return description, title\n",
        "\n",
        "    def crawl_news_data(self, user_search_list, user_name):\n",
        "\n",
        "        display_count = 100\n",
        "        kind = 'news'\n",
        "\n",
        "        for search in user_search_list:\n",
        "            self.jsonResult = []\n",
        "            pre_search_text = '\"수주\"'\n",
        "            real_search_text = search\n",
        "            search_text = real_search_text + ' ' + pre_search_text\n",
        "            jsonSearch = self.get_naver_json(kind, search_text, 1, display_count)\n",
        "\n",
        "            while ((jsonSearch != None) and (jsonSearch['display'] != 0)):\n",
        "\n",
        "                for post in jsonSearch['items']:\n",
        "                    self.process_data(post, self.jsonResult)\n",
        "\n",
        "                nStart = jsonSearch['start'] + jsonSearch['display']\n",
        "                jsonSearch = self.get_naver_json(kind, search_text, nStart, display_count)\n",
        "\n",
        "            with open(\"%s.json\" % search, 'w', encoding='utf8') as f:\n",
        "                result_json = json.dumps(self.jsonResult, indent=4, sort_keys=True, ensure_ascii=False)\n",
        "                f.write(result_json)\n",
        "\n",
        "            data = pd.read_json(\"%s.json\" % search, encoding='utf-8')\n",
        "            data = pd.DataFrame(data, columns = ['날짜', '제목', '기사요약', '링크'])\n",
        "\n",
        "            data['담당자'] = user_name\n",
        "            data['사업체명'] = real_search_text\n",
        "\n",
        "            self.total_data = pd.concat([self.total_data, data], axis=0)\n",
        "            del self.jsonResult\n",
        "\n",
        "        jsonFileList = glob.glob('*.json')\n",
        "        for jsonFile in jsonFileList:\n",
        "            os.remove(jsonFile)\n",
        "        print('-' * 50)\n",
        "        print('수집 완료!')\n",
        "        print('-' * 50)\n",
        "\n",
        "    def save_to_excel(self):\n",
        "        filename = os.path.join(path,\"경제조사과_건설수집자료\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\") + '.xlsx')\n",
        "        self.total_data.reset_index(drop=True)\n",
        "        self.total_data = self.total_data[['담당자','사업체명','날짜', '제목', '기사요약', '링크']]\n",
        "        self.total_data.to_excel(filename, index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    c = time.time()\n",
        "    crawl = gunseol_crawl(\"/content/gdrive/My Drive/Colab Notebooks/건설경기동향조사/건설명부(202010).csv\", multiprocessing=False)\n",
        "    d = time.time()\n",
        "    print(\"crawling time is : %d 초\" % (d-c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woTl_m3Lroj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-bert\n",
        "!pip install keras_radam\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.layers import Add, Lambda, Dense, Embedding, Bidirectional, LSTM, Concatenate, BatchNormalization, Dropout, GlobalMaxPooling1D\n",
        "from keras.layers import Reshape, Activation\n",
        "from keras import Input, Model\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import shutil\n",
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "from tqdm import tqdm\n",
        "\n",
        "import codecs\n",
        "from keras_bert import Tokenizer\n",
        "import re\n",
        "import pickle\n",
        "from keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths\n",
        "from keras_radam import RAdam\n",
        "import keras as keras\n",
        "import keras.backend as K\n",
        "\n",
        "path = \"gdrive/My Drive/Colab Notebooks/건설경기동향조사\"\n",
        "filename = \"경제조사과_건설수집자료2020-02-17-154045.xlsx\"\n",
        "\n",
        "test_naver = pd.read_excel(os.path.join(path, filename))\n",
        "\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks, ignore)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "class inherit_Tokenizer(Tokenizer):\n",
        "  def _tokenize(self, text):\n",
        "        if not self._cased:\n",
        "            \n",
        "            \n",
        "            text = text.lower()\n",
        "        spaced = ''\n",
        "        for ch in text:\n",
        "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
        "                spaced += ' ' + ch + ' '\n",
        "            elif self._is_space(ch):\n",
        "                spaced += ' '\n",
        "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
        "                continue\n",
        "            else:\n",
        "                spaced += ch\n",
        "        tokens = []\n",
        "        for word in spaced.strip().split():\n",
        "            tokens += self._word_piece_tokenize(word)\n",
        "        return tokens\n",
        "\n",
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices = []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
        "        indices.append(ids)\n",
        "        \n",
        "    items = indices\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)]\n",
        "\n",
        "def predict_load_data(x):\n",
        "    data_df = x \n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x\n",
        "\n",
        "!pip install keras-bert\n",
        "\n",
        "try:\n",
        "  os.makedirs(\"etri_bert\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  copytree(os.path.join(path,\"etri_bert\"), \"etri_bert\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS=2\n",
        "LR=1e-5\n",
        "\n",
        "pretrained_path =\"etri_bert\"\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "vocab_path = os.path.join(pretrained_path, 'etri_vocab.txt')\n",
        "\n",
        "DATA_COLUMN = \"제목\"\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        if \"_\" in token:\n",
        "          token = token.replace(\"_\",\"\")\n",
        "          token = \"##\" + token\n",
        "        token_dict[token] = len(token_dict)\n",
        "\n",
        "tokenizer = inherit_Tokenizer(token_dict)\n",
        "\n",
        "\n",
        "\n",
        "layer_num = 12\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=True,\n",
        "    trainable=True,\n",
        "    seq_len=SEQ_LEN,)\n",
        "\n",
        "\n",
        "def get_gunseol_model(model):\n",
        "  inputs = model.inputs[:2]\n",
        "  dense = model.layers[-3].output\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                              name = 'real_output')(dense)\n",
        "\n",
        "\n",
        "\n",
        "  semi_bert_model = keras.models.Model(inputs, outputs)\n",
        "  semi_bert_model.compile(\n",
        "      optimizer=RAdam(learning_rate=0.00001, weight_decay=0.0025),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  \n",
        "  return semi_bert_model\n",
        "\n",
        "\n",
        "bert_model = get_gunseol_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnEuJ8eQAVVl",
        "colab_type": "code",
        "outputId": "7832631b-ac34-4527-e07b-5caeac128aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import datetime\n",
        "bert_model.load_weights(os.path.join(path, \"semi_bert_model.h5\"))\n",
        "test_naver['제목'] = test_naver['제목'].astype(str) + test_naver['기사요약'].astype(str)\n",
        "test_naver_x = predict_load_data(test_naver)\n",
        "\n",
        "naver_predict = bert_model.predict(test_naver_x)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5603/5603 [00:02<00:00, 2660.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BI_wXP_E52M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_naver['기사여부'] = np.round(np.squeeze(naver_predict),0)\n",
        "test_naver.head(10)\n",
        "test_naver.to_excel(os.path.join(path, datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\") + \"_필터처리분.xlsx\"), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}